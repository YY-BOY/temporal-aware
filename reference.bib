% ===================================================================
% TEMPORAL CHANGE DETECTION AND REPORT GENERATION - RECENT WORKS (2024-2025)
% ===================================================================

@article{liu2025mlrg,
  title={Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation},
  author={Liu, Ming-Kai and others},
  journal={arXiv preprint arXiv:2502.20056},
  year={2025},
  note={Accepted to CVPR 2025},
  url={https://arxiv.org/abs/2502.20056},
  file={papers/Liu_2025_MLRG_Enhanced_Contrastive_Learning_Multi_View_Longitudinal_CXR_CVPR.pdf},
  abstract={Multi-view longitudinal contrastive learning integrating spatial and temporal information. Achieves 2.3\% BLEU-4 improvement on MIMIC-CXR, 5.5\% F1 improvement on MIMIC-ABN.}
}

@inproceedings{wang2024hergen,
  title={HERGen: Elevating Radiology Report Generation with Longitudinal Data},
  author={Wang, Fuying and Du, Shenghui and Yu, Lequan},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  year={2024},
  organization={Springer},
  url={https://arxiv.org/abs/2407.15158},
  code={https://github.com/HKU-MedAI/HERGen},
  file={papers/Wang_2024_HERGen_History_Enhanced_Radiology_Report_Generation_Longitudinal_ECCV.pdf},
  abstract={History Enhanced framework using group causal transformer for longitudinal data integration with curriculum learning strategy.}
}

@article{kim2024ehrxdiff,
  title={Towards Predicting Temporal Changes in a Patient's Chest X-ray Images based on Electronic Health Records},
  author={Kim, Daeun and others},
  journal={arXiv preprint arXiv:2409.07012},
  year={2024},
  note={CHIL 2025},
  url={https://arxiv.org/abs/2409.07012},
  code={https://github.com/dek924/EHRXDiff},
  file={papers/Kim_2024_EHRXDiff_Predicting_Temporal_Changes_CXR_from_EHR_CHIL.pdf},
  abstract={Diffusion-based framework predicting future CXRs by integrating previous CXRs with EHR tabular data and medical events.}
}

@inproceedings{atici2024tibix,
  title={TiBiX: Leveraging Temporal Information for Bidirectional X-ray and Report Generation},
  author={Atici, Muhammed Furkan and others},
  booktitle={Deep Generative Models Workshop at MICCAI},
  year={2024},
  url={https://arxiv.org/abs/2403.13343},
  code={https://github.com/BioMedIA-MBZUAI/TiBiX},
  file={papers/Atici_2024_TiBiX_Temporal_Bidirectional_XRay_Report_Generation_MICCAI.pdf},
  abstract={Bidirectional generation model leveraging temporal information. Introduces MIMIC-T temporal benchmark dataset.}
}

@article{xie2024cxrtft,
  title={CXR-TFT: Multi-Modal Temporal Fusion Transformer for Predicting Chest X-ray Trajectories},
  author={Xie, Haoran and others},
  journal={arXiv preprint arXiv:2507.14766},
  year={2024},
  url={https://arxiv.org/abs/2507.14766},
  file={papers/Xie_2024_CXR_TFT_MultiModal_Temporal_Fusion_Transformer_CXR_Trajectories.pdf},
  abstract={Multi-modal framework integrating temporally sparse CXR imaging with high-frequency clinical data. Predicts findings with 95\% accuracy 12 hours before next scan.}
}

@article{nguyen2025chestxtranscribe,
  title={ChestX-Transcribe: a multimodal transformer for automated radiology report generation from chest x-rays},
  author={Nguyen, Hung Thinh and others},
  journal={Frontiers in Digital Health},
  volume={7},
  year={2025},
  publisher={Frontiers},
  url={https://www.frontiersin.org/journals/digital-health/articles/10.3389/fdgth.2025.1535168/full},
  abstract={Combines Swin Transformer for visual features with DistilGPT for report generation.}
}

@article{guria2024chestbioxgen,
  title={ChestBioX-Gen: contextual biomedical report generation from chest X-ray images using BioGPT and co-attention mechanism},
  author={Guria, Suvanjan and others},
  journal={Frontiers in Imaging},
  year={2024},
  url={https://www.frontiersin.org/journals/imaging/articles/10.3389/fimag.2024.1373420/full},
  abstract={Leverages BioGPT and co-attention mechanisms for automatic chest X-ray report generation.}
}

% ===================================================================
% SURVEY AND REVIEW PAPERS
% ===================================================================

@article{zhang2025survey,
  title={A survey of deep-learning-based radiology report generation using multimodal inputs},
  author={Zhang, Yue and others},
  journal={Medical Image Analysis},
  year={2025},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S1361841525001744},
  abstract={Comprehensive survey of RRG developments from 2021-2025, focusing on transformer-based and vision-language models.}
}

@article{delrue2023evaluating,
  title={Evaluating progress in automatic chest X-ray radiology report generation},
  author={Delrue, Linde and others},
  journal={Patterns},
  volume={4},
  number={9},
  year={2023},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S2666389923001575},
  doi={10.1016/j.patter.2023.100802}
}

@article{ali2021deep,
  title={Deep learning for chest X-ray analysis: A survey},
  author={Ali, Ahmed and others},
  journal={Medical Image Analysis},
  volume={72},
  pages={102125},
  year={2021},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/pii/S1361841521001717}
}

@article{park2023comprehensive,
  title={Advancements in Radiology Report Generation: A Comprehensive Analysis},
  author={Park, Sungjun and others},
  journal={Bioengineering},
  volume={12},
  number={7},
  pages={693},
  year={2025},
  publisher={MDPI},
  url={https://www.mdpi.com/2306-5354/12/7/693}
}

@article{wang2024vision,
  title={Vision-language models for medical report generation and visual question answering: a review},
  author={Wang, Irene Y. and others},
  journal={Frontiers in Artificial Intelligence},
  year={2024},
  url={https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1430984/full}
}

% ===================================================================
% VISION TRANSFORMERS AND TRANSFORMER-BASED ARCHITECTURES
% ===================================================================

@article{sriram2025gitcxr,
  title={GIT-CXR: End-to-End Transformer for Chest X-Ray Report Generation},
  author={Sriram, Saumitra and others},
  journal={arXiv preprint arXiv:2501.02598},
  year={2025},
  url={https://arxiv.org/abs/2501.02598},
  file={papers/Sriram_2025_GIT_CXR_End_to_End_Transformer_Report_Generation.pdf},
  abstract={End-to-end transformer with ViT-based image encoder and text decoder for report generation.}
}

@article{alsaedi2024csamdt,
  title={CSAMDT: Conditional Self Attention Memory-Driven Transformers for Radiology Report Generation from Chest X-Ray},
  author={Alsaedi, Abdullah and others},
  journal={Applied Sciences},
  year={2024},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC11612068/},
  abstract={Self-attention memory-driven transformer with memory-driven layer normalization for preserving word relationships.}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020},
  url={https://arxiv.org/abs/2010.11929},
  file={papers/Dosovitskiy_2020_ViT_Vision_Transformer_Image_Recognition_at_Scale.pdf},
  note={Vision Transformer (ViT) foundational paper}
}

% ===================================================================
% VISION-LANGUAGE MODELS AND CONTRASTIVE LEARNING
% ===================================================================

@article{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and others},
  journal={arXiv preprint arXiv:2103.00020},
  year={2021},
  url={https://arxiv.org/abs/2103.00020},
  file={papers/Radford_2021_CLIP_Contrastive_Language_Image_Pretraining.pdf},
  note={CLIP - Contrastive Language-Image Pre-training}
}

@article{tiu2022expert,
  title={Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning},
  author={Tiu, Ekin and others},
  journal={Nature Biomedical Engineering},
  year={2022},
  note={MedCLIP}
}

@article{zhang2025vision,
  title={Vision-language foundation models for medical imaging: a review of current practices and innovations},
  author={Zhang, Yixuan and others},
  journal={Biomedical Engineering Letters},
  year={2025},
  publisher={Springer},
  url={https://link.springer.com/article/10.1007/s13534-025-00484-6}
}

% ===================================================================
% CONTRASTIVE LEARNING FOR TEMPORAL MEDICAL IMAGING
% ===================================================================

@article{ouyang2023comet,
  title={Contrast Everything: A Hierarchical Contrastive Framework for Medical Time-Series},
  author={Ouyang, Yihe and others},
  journal={arXiv preprint arXiv:2310.14017},
  year={2023},
  url={https://arxiv.org/abs/2310.14017},
  file={papers/Ouyang_2023_COMET_Hierarchical_Contrastive_Framework_Medical_Time_Series.pdf},
  abstract={COMET - hierarchical framework leveraging data consistencies at all inherent levels in medical time series.}
}

@article{lotzsch2024spatiotemporal,
  title={Spatiotemporal Representation Learning for Short and Long Medical Image Time Series},
  author={Lotzsch, Wiebke and others},
  journal={arXiv preprint arXiv:2403.07513},
  year={2024},
  url={https://arxiv.org/abs/2403.07513},
  file={papers/Lotzsch_2024_Spatiotemporal_Representation_Learning_Medical_Image_Time_Series.pdf},
  abstract={Clip-level contrastive learning with time embedding for encoding varying time intervals.}
}

@article{kim2023local,
  title={Local Spatiotemporal Representation Learning for Longitudinally-consistent Neuroimage Analysis},
  author={Kim, Taehoon and others},
  journal={Advances in Neural Information Processing Systems},
  year={2023},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC10445502/},
  abstract={Local multi-scale spatiotemporal representation learning exploiting self-similarity of learned features.}
}

@article{rashid2024temporal,
  title={Temporal Supervised Contrastive Learning for Modeling Patient Risk Progression},
  author={Rashid, Shahriar and others},
  journal={arXiv preprint},
  year={2024},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC10976929/}
}

% ===================================================================
% RECURRENT NEURAL NETWORKS FOR TEMPORAL SEQUENCES
% ===================================================================

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT Press},
  note={Original LSTM paper}
}

@inproceedings{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and others},
  booktitle={Proceedings of EMNLP},
  year={2014},
  note={Gated Recurrent Units (GRU)}
}

@article{choi2016doctor,
  title={Using recurrent neural network models for early detection of heart failure onset},
  author={Choi, Edward and others},
  journal={Journal of the American Medical Informatics Association},
  year={2017},
  url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5391725/}
}

% ===================================================================
% TEMPORAL SUBTRACTION AND COMPUTER-AIDED DETECTION
% ===================================================================

@article{kano1997digital,
  title={Digital chest radiography: effect of temporal subtraction images on detection accuracy},
  author={Kano, Akira and others},
  journal={Radiology},
  volume={202},
  number={1},
  pages={263--269},
  year={1997},
  publisher={Radiological Society of North America},
  url={https://pubmed.ncbi.nlm.nih.gov/9015072/},
  abstract={Temporal subtraction increases ROC AUC from 0.89 to 0.98 and reduces interpretation time by 19.3\%.}
}

@article{kakeda2011temporal,
  title={Temporal subtraction for detection of solitary pulmonary nodules on chest radiographs: evaluation of a commercially available computer-aided diagnosis system},
  author={Kakeda, Shingo and others},
  journal={Radiology},
  volume={237},
  number={3},
  pages={1040--1049},
  year={2002},
  publisher={Radiological Society of North America},
  url={https://pubmed.ncbi.nlm.nih.gov/12034953/}
}

@article{johkoh2011integration,
  title={Integration of Temporal Subtraction and Nodule Detection System for Digital Chest Radiographs into Picture Archiving and Communication System (PACS): Four-year Experience},
  author={Johkoh, Takeshi and others},
  journal={Korean Journal of Radiology},
  year={2011},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC3043823/},
  abstract={Clinical implementation of temporal subtraction and CAD systems into PACS.}
}

@article{doi2007computer,
  title={Computer-aided diagnosis in medical imaging: historical review, current status and future potential},
  author={Doi, Kunio},
  journal={Computerized Medical Imaging and Graphics},
  volume={31},
  number={4-5},
  pages={198--211},
  year={2007},
  publisher={Elsevier},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC1955762/}
}

@article{li2018computer,
  title={Computer-aided detection in chest radiography based on artificial intelligence: a survey},
  author={Li, Ziyue and others},
  journal={BioMedical Engineering OnLine},
  volume={17},
  number={1},
  pages={113},
  year={2018},
  publisher={BioMed Central},
  url={https://biomedical-engineering-online.biomedcentral.com/articles/10.1186/s12938-018-0544-y}
}

% ===================================================================
% DATASETS AND BENCHMARKS
% ===================================================================

@article{johnson2019mimic,
  title={MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports},
  author={Johnson, Alistair EW and others},
  journal={Scientific data},
  volume={6},
  number={1},
  pages={317},
  year={2019},
  publisher={Nature Publishing Group},
  url={https://physionet.org/content/mimic-cxr/2.1.0/},
  abstract={371,920 chest X-rays from 227,943 imaging studies and 65,079 patients with date shifts preserving relative chronology.}
}

@article{boecking2022mscxrt,
  title={Making the Most of Text Semantics to Improve Biomedical Vision--Language Processing},
  author={Boecking, Benedikt and others},
  journal={arXiv preprint arXiv:2204.09817},
  year={2022},
  url={https://physionet.org/content/ms-cxr-t/1.0.0/},
  file={papers/Boecking_2022_MS_CXR_T_Text_Semantics_Biomedical_Vision_Language_Processing.pdf},
  note={MS-CXR-T: Microsoft temporal dataset with progression labels for 5 findings across 3 classes (Improving, Stable, Worsening)}
}

@article{demner2016preparing,
  title={Preparing a collection of radiology examinations for distribution and retrieval},
  author={Demner-Fushman, Dina and others},
  journal={Journal of the American Medical Informatics Association},
  volume={23},
  number={2},
  pages={304--310},
  year={2016},
  publisher={Oxford University Press},
  note={IU X-Ray dataset: 7,470 chest X-ray images with 3,955 reports},
  url={https://openi.nlm.nih.gov/}
}

@article{irvin2019chexpert,
  title={CheXpert: A large chest radiograph dataset with uncertainty labels and expert comparison},
  author={Irvin, Jeremy and others},
  journal={arXiv preprint arXiv:1901.07031},
  year={2019},
  url={https://stanfordmlgroup.github.io/competitions/chexpert/},
  file={papers/Irvin_2019_CheXpert_Large_Chest_Radiograph_Dataset_Uncertainty_Labels.pdf},
  abstract={Over 200,000 CXRs from 65,240 patients with 14 observations using rule-based labeler.}
}

@article{bustos2020padchest,
  title={PadChest: A large chest x-ray image dataset with multi-label annotated reports},
  author={Bustos, Aurelia and others},
  journal={Medical Image Analysis},
  volume={66},
  pages={101797},
  year={2020},
  publisher={Elsevier},
  url={https://www.sciencedirect.com/science/article/abs/pii/S1361841520301614},
  abstract={160,000+ CXR images, 27\% hand-labeled with 174 findings and 19 diagnoses, rest labeled via NLP. Spanish dataset.}
}

@inproceedings{wang2017chestxray14,
  title={ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases},
  author={Wang, Xiaosong and others},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2097--2106},
  year={2017},
  note={112,000+ CXR scans from 30,000+ patients. Labels from automated NLP tools.},
  url={https://cloud.google.com/healthcare-api/docs/resources/public-datasets/nih-chest}
}

@article{nguyen2021vindrcxr,
  title={VinDr-CXR: An open dataset of chest X-rays with radiologist's annotations},
  author={Nguyen, Ha Q and others},
  journal={Medical Imaging with Deep Learning},
  year={2021},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC9300612/}
}

% ===================================================================
% EVALUATION METRICS
% ===================================================================

@article{jain2021radgraph,
  title={RadGraph: Extracting Clinical Entities and Relations from Radiology Reports},
  author={Jain, Saahil and others},
  journal={arXiv preprint arXiv:2106.14463},
  year={2021},
  url={https://arxiv.org/abs/2106.14463},
  file={papers/Jain_2021_RadGraph_Extracting_Clinical_Entities_Relations_Radiology_Reports.pdf},
  abstract={Dataset of entities and relations from 500 radiology reports (14,579 entities, 10,889 relations). RadGraph F1 metric.}
}

@article{smit2020chexbert,
  title={CheXbert: Combining Automatic Labelers and Expert Annotations for Accurate Radiology Report Classification},
  author={Smit, Akshay and others},
  journal={arXiv preprint arXiv:2004.09167},
  year={2020},
  url={https://arxiv.org/abs/2004.09167},
  file={papers/Smit_2020_CheXbert_Automatic_Labelers_Expert_Annotations_Radiology_Reports.pdf},
  note={CheXbert F1 score for clinical efficacy evaluation}
}

@article{vu2024green,
  title={GREEN: Generative Radiology Report Evaluation and Error Notation},
  author={Vu, Manh-Huy and others},
  journal={arXiv preprint},
  year={2024},
  url={https://stanford-aimi.github.io/green.html},
  abstract={Error detection metric with 0.79 correlation with radiologist error counts.}
}

@article{papineni2002bleu,
  title={BLEU: a method for automatic evaluation of machine translation},
  author={Papineni, Kishore and others},
  booktitle={Proceedings of ACL},
  pages={311--318},
  year={2002},
  note={BLEU metric for n-gram overlap}
}

@inproceedings{lin2004rouge,
  title={ROUGE: A package for automatic evaluation of summaries},
  author={Lin, Chin-Yew},
  booktitle={Text summarization branches out},
  pages={74--81},
  year={2004},
  note={ROUGE metric for recall-oriented evaluation}
}

@inproceedings{vedantam2015cider,
  title={CIDEr: Consensus-based image description evaluation},
  author={Vedantam, Ramakrishna and others},
  booktitle={Proceedings of CVPR},
  pages={4566--4575},
  year={2015},
  note={CIDEr metric for consensus-based evaluation}
}

@inproceedings{denkowski2014meteor,
  title={Meteor universal: Language specific translation evaluation for any target language},
  author={Denkowski, Michael and Lavie, Alon},
  booktitle={Proceedings of EACL Workshop on Statistical Machine Translation},
  year={2014},
  note={METEOR metric with semantic similarity}
}

% ===================================================================
% LONGITUDINAL DATA AND SEMANTIC SIMILARITY
% ===================================================================

@article{pang2023longitudinal,
  title={Longitudinal data and a semantic similarity reward for chest X-ray report generation},
  author={Pang, Haowei and others},
  journal={arXiv preprint arXiv:2307.09758},
  year={2023},
  url={https://arxiv.org/abs/2307.09758},
  file={papers/Pang_2023_Longitudinal_Data_Semantic_Similarity_Reward_CXR_Report_Generation.pdf},
  abstract={Leverages longitudinal data and semantic similarity reward mirroring radiologist workflow.}
}

@article{yu2023utilizing,
  title={Utilizing Longitudinal Chest X-Rays and Reports to Pre-Fill Radiology Reports},
  author={Yu, Tom and others},
  journal={AMIA Summits on Translational Science Proceedings},
  year={2023},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC10370215/},
  abstract={Multi-modal study data (CXR images + reports) for longitudinal context.}
}

% ===================================================================
% COLLABORATION WITH CLINICIANS AND CLINICAL VALIDATION
% ===================================================================

@article{tu2024towards,
  title={Towards radiologist-level cancer risk assessment in mammography screening with deep learning},
  author={Tu, Rachel and others},
  journal={Nature Medicine},
  year={2024},
  url={https://www.nature.com/articles/s41591-024-03302-1},
  abstract={Collaboration between clinicians and vision-language models in radiology report generation.}
}

@article{irvin2024generative,
  title={Generative Artificial Intelligence for Chest Radiograph Interpretation in the Emergency Department},
  author={Irvin, Jeremy and others},
  journal={JAMA Network Open},
  volume={6},
  number={10},
  year={2023},
  url={https://jamanetwork.com/journals/jamanetworkopen/fullarticle/2810195},
  abstract={Clinical validation of AI for chest radiograph interpretation in emergency settings.}
}

% ===================================================================
% REINFORCEMENT LEARNING FOR REPORT GENERATION
% ===================================================================

@article{liu2024improving,
  title={Improving Radiology Report Generation Quality and Diversity through Reinforcement Learning and Text Augmentation},
  author={Liu, Jingyi and others},
  journal={arXiv preprint},
  year={2024},
  url={https://pmc.ncbi.nlm.nih.gov/articles/PMC11048060/},
  abstract={RL with RadGraph as reward metric surpassing BLEU4, ROUGE-L, F1CheXbert benchmarks.}
}

@article{miura2021improving,
  title={CADxReport: Chest x-ray report generation using co-attention mechanism and reinforcement learning},
  author={Miura, Yasuhide and others},
  journal={Computer Methods and Programs in Biomedicine},
  year={2021},
  url={https://pubmed.ncbi.nlm.nih.gov/35585727/}
}

% ===================================================================
% ADDITIONAL RECENT ARCHITECTURES AND METHODS
% ===================================================================

@article{wang2024llmrg4,
  title={LLM-RG4: Flexible and Factual Radiology Report Generation across Diverse Input Contexts},
  author={Wang, Zhuhao and Sun, Yihua and Li, Zihan and Yang, Xuan and Chen, Fang and Liao, Hongen},
  journal={arXiv preprint arXiv:2412.12001},
  year={2024},
  url={https://arxiv.org/abs/2412.12001},
  file={papers/Wang_2024_LLM_RG4_Flexible_Factual_Radiology_Report_Generation.pdf},
  abstract={Large language model approach for flexible report generation across four input scenarios (single/multi-view with/without longitudinal data). Introduces MIMIC-RG4 dataset with adaptive token fusion module and token-level loss weighting strategy to minimize input-agnostic hallucinations.}
}

@article{bae2023ehrxqa,
  title={EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images},
  author={Bae, Seongsu and others},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  url={https://physionet.org/content/ehrxqa/1.0.0/},
  abstract={Multi-modal VQA dataset based on MIMIC-CXR combining EHR and imaging data.}
}

@article{zhang2024rexrank,
  title={ReXrank: A Public Leaderboard for AI-Powered Radiology Report Generation},
  author={Zhang, Xiaoman and Zhou, Hong-Yu and Rajpurkar, Pranav and others},
  journal={arXiv preprint arXiv:2411.15122},
  year={2024},
  url={https://arxiv.org/abs/2411.15122},
  website={https://rexrank.ai},
  file={papers/Zhang_2024_ReXrank_Public_Leaderboard_AI_Radiology_Report_Generation.pdf},
  abstract={Public leaderboard and challenge for assessing AI-powered radiology report generation. Incorporates ReXGradient (10,000 studies) and three public datasets (MIMIC-CXR, IU-Xray, CheXpert Plus) with 8 evaluation metrics.}
}

% ===================================================================
% END OF BIBLIOGRAPHY
% ===================================================================